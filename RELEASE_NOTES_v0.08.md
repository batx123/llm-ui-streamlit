# Release Notes v0.08

## Improvements
- Increased default `max_tokens` for LLM responses from 64 to 512, reducing the chance of incomplete or cut-off answers.
- Users can still adjust this value in the UI as needed.

## Versioning
- Tagged as v0.08

---
See README.md and FAQ.md for full details.
